# Yang: let's give a try see if I could make something work for one datapoint
import pandas as pd
from datasets import load_dataset
import transformers
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
# from dyna_gym.pipelines import uct_for_hf_transformer_pipeline
import subprocess
import shlex
import zipfile
import torch
from databench_eval import Runner, Evaluator, utils

# load data and make a dataset containing the first data point
semeval_dev_qa = load_dataset("cardiffnlp/databench", name="semeval", split="dev")
df_all = pd.read_parquet('all.parquet')
qa = semeval_dev_qa.select([0])
gold_ans = qa[0]['sample_answer']

tokenizer = AutoTokenizer.from_pretrained("stabilityai/stable-code-3b")
model = AutoModelForCausalLM.from_pretrained("stabilityai/stable-code-3b", torch_dtype="auto", )
model.cuda()


def call_model(prompts):
    """
  tokenize prompt, model generate;
  prompts: str
  """
    # print("*" * 50)
    inputs = tokenizer(prompts, return_tensors="pt").to(model.device)
    tokens = model.generate(
        **inputs,
        max_new_tokens=128,
        temperature=0.2,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id)
    result = tokenizer.decode(tokens[0], skip_special_tokens=True)
    print(result)
    return result


def prompt_generator(row):
    question = row['question'][0]
    df = df_all
    prompt = f"""
# TODO: complete the following function in one line. It should give the answer to: How many rows are there in this dataframe?
def example(df: pd.DataFrame) -> int:
    df.columns=["A"]
    return df.shape[0]

# TODO: complete the following function in one line. It should give the answer to: {question}
def answer(df: pd.DataFrame) -> {row["type"][0]}:
    df.columns = {list(df.columns)}
    return"""
    return prompt


def post_process(response: str, dataset: str):
    """
    process and execute the code generated by model
    response: the textual completion generated by the model
    dataset: the dataset we want to query, in this one data point case is "050_ING" which is df_all(I load it locally) above;
    """

    """
    Below is how this process actually look like
    df = pd.DataFrame({1:"aa",2:"bb"})
    response = "def print():\n return print(hello world)"
    global ans
    def answer(df):
        return response.split("return")[2].split("\n")[0].strip().replace("[end of text]", "")
    ans = answer(df)  #ans is the predicted textual answer of the question by model, which should match with the gold answer;
    """

    try:
        # df = loader(dataset)
        df = dataset
        lead = """
def answer(df):
    return """
        exec(
            "global ans\n"
            + lead
            + response.split("return")[2]
            .split("\n")[0]
            .strip()
            .replace("[end of text]", "")
            + f"\nans = answer(df)"
        )
        # no true result is > 1 line atm, needs 1 line for txt format
        return ans.split("\n")[0] if "\n" in str(ans) else ans
    except Exception as e:
        return f"__CODE_ERROR__: {e}"


def compare(value, truth):
    return str(value).strip == str(truth).strip()


def run():
    prompt = prompt_generator(qa)
    completion = call_model(prompt)
    predicted_answer = post_process(completion, df_all)
    return compare(predicted_answer, qa[0]['sample_answer'])


#######################################################################
def compute_reward(value, truth):
    if compare(value, truth):
        return
