import transformers
from dyna_gym.pipelines import uct_for_hf_transformer_pipeline
from transformers import AutoModelForCausalLM, AutoTokenizer
import pandas as pd
import subprocess
import shlex
import zipfile
import torch
from databench_eval import Runner, Evaluator, utils
from datasets import load_dataset
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from databench_eval import Runner, Evaluator, utils
import os

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

############################################ set up the model##############################################
model_name = "stabilityai/stable-code-3b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
vocab_size = tokenizer.vocab_size
horizon = 250  # maximum number of steps / tokens to generate in each episode

##################################### load one data point#################################################
print("load data", "-" * 30)
# load data and make a dataset containing the first data point
semeval_dev_qa = load_dataset("cardiffnlp/databench", name="semeval", split="dev")
df_all = pd.read_parquet(r'..\data-frame\all.parquet')
qa = semeval_dev_qa.select([0])
gold_ans = qa[0]['sample_answer']
print(qa)
print("load data finished", "-" * 30)


##################################### run one datapoint #################################################
def call_model(prompts):
    """
    tokenize prompt, model generate;
    prompts: str
    """
    # print("*" * 50)
    inputs = tokenizer(prompts, return_tensors="pt").to(model.device)
    tokens = model.generate(
        **inputs,
        max_new_tokens=128,
        temperature=0.2,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id)
    result = tokenizer.decode(tokens[0], skip_special_tokens=True)
    return result


def prompt_generator(row):
    question = row['question']
    df = df_all
    prompt = f"""
# TODO: complete the following function in one line. It should give the answer to: How many rows are there in this dataframe?
def example(df: pd.DataFrame) -> int:
    df.columns=["A"]
    return df.shape[0]

# TODO: complete the following function in one line. It should give the answer to: {question}
def answer(df: pd.DataFrame) -> {row["type"]}:
    df.columns = {list(df.columns)}
    return"""
    return prompt


def post_process(response: str, dataset: str):
    """
    process and execute the code generated by model
    response: the textual completion generated by the model
    dataset: the dataset we want to query, in this one data point case is "050_ING" which is df_all(I load it locally) above;
    """

    """
    Below is how this process actually look like
    df = pd.DataFrame({1:"aa",2:"bb"})
    response = "def print():\n return print(hello world)"
    global ans
    def answer(df):
        return response.split("return")[2].split("\n")[0].strip().replace("[end of text]", "")
    ans = answer(df)  #ans is the predicted textual answer of the question by model, which should match with the gold answer;
    """

    try:
        # df = loader(dataset)
        df = dataset
        lead = """
def answer(df):
    return """
        exec(
            "global ans\n"
            + lead
            + response.split("return")[2]
            .split("\n")[0]
            .strip()
            .replace("[end of text]", "")
            + f"\nans = answer(df)"
        )
        # no true result is > 1 line atm, needs 1 line for txt format
        return ans.split("\n")[0] if "\n" in str(ans) else ans
    except Exception as e:
        return f"__CODE_ERROR__: {e}"


def massage_response(response: str):
    return response.split("return")[2].split("\n")[0].strip().replace("[end of text]", "")


def compare(value, truth):
    return str(value).strip == str(truth).strip()


print("Run model", "-" * 30)
prompt = prompt_generator(qa[0])
completion = call_model(prompt)
code_ = massage_response(completion)
result_ = post_process(completion, df_all)
print(f"Prompt is: {prompt}")
print(f"Completion is:  {completion}")
print(f"Actual code is: {code_}")
print(f"Result is:  {result_}")
print("Run model", "-" * 30)


##################################### set up the RL process #################################################
def reward_function(sentence):
    """
    if (get correct NL answer) -> 1
    else -> 0
    """
    if compare(sentence, qa[0]['answer']):
        return 1
    else:
        return 0


# arguments for the UCT agent
uct_args = dict(
    rollouts=3,
    gamma=1.,
    width=5,
    alg='uct',  # or p_uct
)

# will be passed to huggingface model.generate()
model_generation_args = dict(
    top_k=3,
    top_p=0.9,
    do_sample=True,
    temperature=0.2,
)

pipeline = uct_for_hf_transformer_pipeline(
    model=model,
    tokenizer=tokenizer,
    horizon=horizon,
    reward_func=reward_function,
    uct_args=uct_args,
    model_generation_args=model_generation_args,
    should_plot_tree=True,  # plot the tree after generation
)

input_str = prompt_generator(qa[0])
outputs = pipeline(input_str=input_str)

for text, reward in zip(outputs['texts'], outputs['rewards']):
    print("==== Text ====")
    print(text)
    print("==== Reward:", reward, "====")
    print()
