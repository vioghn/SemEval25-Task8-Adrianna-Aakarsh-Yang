% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{bbm}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{TeamName at SemEval-2025 Task 8: Question-Answering over Tabular Data}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
SemEval-2025 Task 8 focuses on querying tabular data using code generated by large language models(LLMs). The dataset for this task consists of natural language questions, relevant column information and  corresponding answers. The challenge primarily lies in the multiple-step reasoning process of transforming natural language queries into executable code. This challenge is enlarged by the limitations of current approaches, such as Chain-of-Thought reasoning, which struggle with complex multi-step reasoning path because of the difficulty of evaluation of intermediate steps. To address these limitations, we propose a Monte Carlo Tree Search (MCTS) algorithm to simulate the reasoning process during code generation. It includes rewarding, regret and planning mechanism, which enables LLMs to explore multiple reasoning paths, evaluate outcomes and refine the code.


\end{abstract}

\section{Introduction}
The task aims to implement a code-generating large language model(LLM) to query tabular data. The goal is to implement the generated code Python to answer questions with regard to a given data frame. The training data provided by the organizers contains 960 data points. Each includes a natural language question, the target answer and its data type, relevant columns with their data types, and associated data frame. The aim is to predict the answer to each question based on the generated Python code, which extracting the required information from the data frame\citep{oses-grijalba-etal-2024-question}. \footnote{Our code is available at:  \url{https://github.com/HuixinYang/SemEval25-Task8-Adrianna-Aakarsh-Yang}} 

We propose treating LLM as a step-by-step reasoning agent which decomposes the task into subtasks, making look-ahead planning and iteratively debugs to reach the goal. In practice a programmer usually write several lines of code to narrow down the data to a single column or several columns, and might also include debugging along the way. This differs from the reasoning path of current code LLMs trained based on auto regressive reasoning manner like Chain-of-Thought, which follows linear, greedy reasoning paths[slots for code llm]. This discrepancy introduces challenges in terms of evaluation of intermediate states, lack of planning mechanism based on back propagated rewarding of intermediate reasoning states, and also lack of mechanism to explore the promising reasoning paths\citep{zhang2023planninglargelanguagemodels}. 

To address these challenges, we propose a Monte Carlo Tree Search(MCTS) algorithm to simulate the planning and debugging process. MCTS iteratively build a branching reasoning tree by selecting the most promising reasoning steps based on the potential rewarding and the balance between exploration and exploitation. This approach enables LLM to refine its reasoning path and improve its abilities to reach accurate solutions\citep{gao2024interpretablecontrastivemontecarlo}.

\section{Related Work}
LLMs have remarkable reasoning abilities in tackling complex mathematical, logical and programming tasks\citep{openai2024gpt4technicalreport}. Approaches have been focused on simulating the step-by-step decomposition of reasoning process, which aim to mimic human cognitive processes by generating coherent reasoning path that leads to the correct outcomes\citep{wei2023chainofthoughtpromptingelicitsreasoning}. Planning has emerged as a crucial aspect of LLM reasoning, especially for tasks which require multi-step problem solving and long-term coherence. Unlike traditional autoregressive approaches, in which each steps is solely based on the previous tokens or sequences, planning introduces a forward-looking strategy which evaluate multiple possible approaches and select the most promising ones\citep{hao2024llmreasonersnewevaluation}. In this section, we formalize and compare several planning approaches to illustrate their impact on improving the reasoning capabilities of LLMs. 


\subsection{CoT and its variant}  
Chain-of-Thought approach follows an implicit greedy planning policy. Each intermediate state is sampled by $ a_i \sim p_{LLM}(a_0 \ldots a_{i-1}\mid x) $, and the solution is sampled by $y\sim p_{LLM}(y\mid x, a_0 \ldots a_{n})$\citep{wei2023chainofthoughtpromptingelicitsreasoning}. To increase the robustness of this greedy approach, Chain-of-Thought with self consistency(CoT-SC) introduced the self-consistent planning by selecting the most frequent path among set of reasoning chains. It is based on the intuition that the correct answer of a complex reasoning problem might from multiple reasoning paths\citep{wang2023selfconsistencyimproveschainthought}. CoT-SC iteratively samples $i$ CoT independent reasoning chains by $[a_0\ldots a_n, y] ^i \sim p_{\text{CoT}, LLM}(x)$, and selects the most frequent path $\arg\max_{a_0 \ldots a_n, y} \sum_{i=0}^{n} \mathbbm{1}\{ y_i = y \}$. The underlying autoregressive mechanism of these approaches introduces the difficulties of evaluation the intermediate reasoning steps and exploration of more promising path from the less likely steps\citep{sprague2024cotcotchainofthoughthelps}. 

Its application in code generating tasks, the challenge of intermediate steps is more pronounced due to the non-deterministic property of code generation\citep{Ouyang_2024}, which introduces difficulties of evaluating the intermediate steps to reduce false positive. Approaches has been done in breaking down the task and interact between code and natural language multiple times until it passes the unit tests\citep{ding2024semcodertrainingcodelanguage}.Besides this back-and-forth process, models are also encouraged to perform deeper reasoning by introducing increasing difficulty in the tasks which aims at ultimately improving their ability to generate robust code\citep{luo2023wizardcoderempoweringcodelarge}.


\subsection{Tree reasoning}
Inspired by deliberate reasoning, Tree-of-Thought(ToT) reasoning structures the reasoning paths as a branching tree, which allows parallel reasoning across paths\citep{yao2023treethoughtsdeliberateproblem}. However, it locally selects the next state greedily based on votes or score from the model, which is $\arg\max_{s}p_{value,LLM}(v\mid s)$ or $\arg\max_{s} p_{LLM}^{\text{vote}}(s)$. Monte Carlo Tree Reasoning with Planning improves this by allowing LLM to plan a reasoning path based on the world model and back propagated rewards which iteratively build a tree policy, which enables a human-like deliberate reasoning strategies\citep{hao2023reasoninglanguagemodelplanning}. MCTS can handle the intermediate steps challenges by balancing exploration and exploitation, offering a more robust approach compared to greedy methods.

\section{Our Approach}
We adopt the Reasoning via Planning(RAP) framework to build a Monte Carlo Tree Search algorithm for our reasoning process\citep{hao2023reasoninglanguagemodelplanning}. In this framework, each node represent a state, and each edge denotes an action from current state to the next. The aim is to guide the LLM to search for a path which maximizes accumulated rewarding, $\sum_{t=0}^{n} r(s_{t},a{t})$. Each iteration of look-ahead planning contains two phases. First, the algorithm selects the next node roughly proportion to its back-propagated rewarding along the visited path, and also adjusted with an exploration term(see session 3.1) to balance the exploration and exploitation. Second, once reaching the leaf node, the next node is selected either randomly or greedily. The first phase expanding the tree step-by-step which enables the simulation through look-ahead planning that is not solely greedy as in autoregressive approach but instead based on estimating the expected future reward of taking action \(a\) in state \(s\),  \(Q(s,a)\), which is updated in the backpropagation phase of each iteration. The following is an detailed illustration about its steps. 
\subsection{Selection} This process starts at the root node and recursively select its child node until a leaf node is reached. It is based on Upper Confidence Bound for Trees (UCT) to balance exploration and exploitation\citep{10.1007/11871842_29}. 
\begin{equation}
    UCT(s) = \hat{Q}(s,a) + C \sqrt{\frac{\ln N(s)}{N(c(s,a))}}
\end{equation}

\subsection{Expansion}New child node is selected by sampling \(d\) possible reasoning path.
\subsection{Simulation}The model samples randomly to simulate the roll-out process until a terminal node is reached with default policy. 
\subsection{Back Propagation}Once a terminal node is reached, the rewarding is back propagated up to the tree to update the values of the visited nodes to refine the tree policy.



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{mid-references}

\appendix
\section{Example Appendix}
\label{sec:appendix}

This is an appendix.

\end{document}
